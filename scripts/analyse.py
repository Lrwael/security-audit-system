import csv
import subprocess

# Function to read a CSV file and generate a prompt
def generate_prompt_from_csv(csv_file):
    prompt = f"The file `{csv_file}` contains the results of a Nessus vulnerability scan.\n\n"
    prompt += "Below are the first 5 rows of the data for your review:\n\n"
    
    with open(csv_file, newline='', encoding='utf-8') as file:
        reader = csv.reader(file)
        headers = next(reader)
        prompt += ", ".join(headers) + "\n"
        for i, row in enumerate(reader):
            if i >= 5:  # Only read the first 5 rows
                break
            prompt += ", ".join(row) + "\n"
    
    prompt += (
        "\nUsing the data provided:\n"
        "1. Identify and analyze the critical, high, and medium severity vulnerabilities.\n"
        "2. Explain the potential risks associated with these vulnerabilities.\n"
        "3. Provide detailed, actionable recommendations to mitigate each vulnerability.\n"
        "4. Highlight any potential patterns or trends in the scan data that could indicate broader issues.\n"
        "Ensure your analysis is clear and concise, using professional language tailored for a cybersecurity report.\n"
    )
    return prompt

# Function to interact with Ollama locally via command line
def query_ollama(prompt, model="llama3.1"):
    try:
        # Run the Ollama command to generate the response with the model
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt, text=True, capture_output=True, check=True
        )
        # The output of the command contains the response generated by the model
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error running the Ollama command: {e}"

# Main function
def main():
    csv_file = "scan_5test.csv"  # Replace with the path to your CSV file
    output_file = "analysis_scan.txt"

    # Generate the prompt from the CSV file
    prompt = generate_prompt_from_csv(csv_file)
    print(f"Generated Prompt:\n{prompt}")

    # Obtain the model's response
    response = query_ollama(prompt)
    print(f"Model Response:\n{response}")

    # Save the response to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(response)
    print(f"Analysis saved in: {output_file}")

if __name__ == "__main__":
    main()
